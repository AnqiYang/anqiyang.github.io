<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Anqi Yang</title>

    <meta name="author" content="Anqi Yang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Anqi Yang
                </p>
                <p>I'm software engineer at Google Cloud AI, working on multimedia generation and editing. I obtained my PhD from Carnegie Mellon University</a>, advised by
                  <a href="https://users.ece.cmu.edu/~saswin/">Prof. Aswin C. Sankaranarayanan</a>.
                  My thesis has been focused on pushing the frontier of mobile photography,
                  by jointly design camera optics, sensor, and efficient deep restoration algorithms.
                  I have won <a href="https://imaging.cs.cmu.edu/featured-content/iccp-2021-award/">best paper award at ICCP</a> in 2021 
                  and <a href="https://blog.google/around-the-globe/google-asia/celebrating-next-generation-women-technologists-asia-pacific/">Google Anita Borg Scholarship</a> in 2016.
                </p>
                <!-- <p>
                  My research interest includes:
                  <li>novel cameras design, such as under-display cameras</li>
                  <li>efficient deep learning for camera ISP and downstream tasks</li>
                  <li>extend the capability of photography using diffusion models</li>
                </p> -->
                <p>
                  Prior to that, I obtained my M.S. degree from the Robotics Institute at CMU and my 
                  <a href="https://www.ri.cmu.edu/publications/3d-object-detection-from-ct-scans-using-a-slice-and-fuse-approach/">M.S. thesis</a>
                  was on 3D object detection, segmentation, and classfication for dense 3D volumes.
                  I obtained my B.Eng. degree from School of Software Engineering at Tongji University, working with 
                  <a href="https://scholar.google.com/citations?user=8VOk_S4AAAAJ&hl=zh-CN">Prof. Lin Zhang</a>.
                </p>

                <p>
                  I have spent wonderful summers as research interns at <a href="https://labs.google/">Google Labs</a>, 
                  <a href="https://sra.samsung.com/research-area/mobile-processor-innovation/">Samsung Research America</a>, and 
                  <a href="https://www.cs.toronto.edu/~taati/">University of Toronto</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:angelyang636@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/Anqi_Resume_latest.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=xNVVAhgAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/anqi-yang-086883107/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/avatar.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/avatar.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    

    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='denoiser_image'><video  width=100% muted autoplay loop>
          <source src="images/denoiser.jpg" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/denoiser.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('denoiser_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('denoiser_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <span class="papertitle">Fast Video Denoising with Uncalibrated Webcams</span>
        </a>
        <br>
        <strong>Anqi Yang</strong>,
        Zhichao Yin,
        Rahul Garg,
        Xin Tong
        <br>
        <a href="data/denoiser_tech_report_23.pdf"><em>tech report</em>, 2023</a>
        <br>
        <p></p>
        <!-- <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
        /
        <a href="http://arxiv.org/abs/2312.05283">arXiv</a> -->
        <em>Key words: Efficient deep learning, test-time adaptation, camera noise modeling</em>
        <p>
        A fast denoising framework that adapts off-the-shelf lightweight neural networks to denoise a wide variety of webcams at test time.
        </p>
      </td>
    </tr>


    <tr onmouseout="udc2_stop()" onmouseover="udc2_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='phase_mask_image'><video  width=100% muted autoplay loop>
          <source src="images/phase_mask.jpg" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/phase_mask.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function udc2_start() {
            document.getElementById('phase_mask_image').style.opacity = "1";
          }

          function udc2_stop() {
            document.getElementById('phase_mask_image').style.opacity = "0";
          }
          udc2_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="http://imagesci.ece.cmu.edu/files/paper/2023/UDCPhase_ICCV23.pdf">
          <span class="papertitle">Designing Phase Masks for Under-Display Cameras</span>
        </a>
        <br>
        <strong>Anqi Yang</strong>,
        Eunhee Kang,
        Hyong-Euk Lee,
        Aswin C. Sankaranarayanan
        <br>
        <em>ICCV</em>, 2023
        <br>
        <a href="ICCV23_UDC/index.html">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=Xlzl3sQ9W0w&t=1s">video</a>
        /
        <a href="http://imagesci.ece.cmu.edu/files/paper/2023/UDCPhase_ICCV23.pdf">paper</a>
        <p></p>
        <em>Key words: Under-display camera, diffractive optical elements, optimization</em>
        <p>
        Under-display cameras is an emerging type of camera that can image a scene through the openings of the display screens.
        This work proposes a novel design of diffractive optical elements that effectively renders the display screen invisible 
        from the camera's perspective and keeps the display quality high at the same time.
        </p>
      </td>
    </tr>
	
	
    <tr onmouseout="udc_stop()" onmouseover="udc_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='udc_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/udc.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/udc.jpg' width="160">
        </div>
        <script type="text/javascript">
          function udc_start() {
            document.getElementById('udc_image').style.opacity = "1";
          }

          function udc_stop() {
            document.getElementById('udc_image').style.opacity = "0";
          }
          udc_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="http://imagesci.ece.cmu.edu/files/paper/2021/UDC_PAMI21.pdf">
			<span class="papertitle">Designing Display Pixel Layouts for Under-Panel Cameras</span>
        </a>
        <br>
        <strong>Anqi Yang</strong>,
        Aswin C. Sankaranarayanan
        <br>
        <em>TPAMI / ICCP</em>, 2021 &nbsp <font color="red"><strong>(Best Paper Award)</strong></font>
        <br>
        <!-- <a>project page</a>
        / -->
        <a href="http://imagesci.ece.cmu.edu/files/paper/2021/UDC_PAMI21.pdf">paper</a>
        /
        <a href="https://www.youtube.com/watch?v=8mIWIOcdhe0">video</a>
        /
        <a href="https://www.ece.cmu.edu/news-and-events/story/2021/06/under-panel-cameras.html">press</a>
        <p></p>
        <em>Key words: Joint optimization of camera and image restoration algorithms, camera ISP, deep learning-based deblurring, hardware prototype, wave optics</em>
        <p>
        The opening patterns on the display plays a crucial role in determining diffractive blur in under-panel cameras.
        This work proposes a suite of modifications to the display opening patterns that significantly increases the
        imaging quality of UDCs.
        </p>
      </td>
    </tr>


<tr onmouseout="slicenets_stop()" onmouseover="slicenets_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='slicenets_image'><video  width=100% height=100% muted autoplay loop>
    <source src="images/slicenets.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video></div>
      <img src='images/slicenets.jpg' width="160">
    </div>
    <script type="text/javascript">
      function slicenets_start() {
        document.getElementById('slicenets_image').style.opacity = "1";
      }

      function slicenets_stop() {
        document.getElementById('slicenets_image').style.opacity = "0";
      }
      slicenets_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="http://imagesci.ece.cmu.edu/files/paper/2021/SliceNets_WACV21.pdf">
      <span class="papertitle">SliceNets --- A Scalable Approach for Object Detection in 3D CT Scans
</span>
    </a>
    <br>
  <strong>Anqi Yang</strong>,
  Feng Pan,
	Vishwanath Saragadam,
	Duy Dao,
	Zhuo Hui,
	Jen-Hao Chang,
	Aswin C. Sankaranarayanan
  <br>
	<em>WACV<em>, 2021
    <br>
    <a href="http://imagesci.ece.cmu.edu/files/paper/2021/SliceNets_WACV21.pdf">paper</a>
    /
    <a href="http://imagesci.ece.cmu.edu/files/paper/2021/SliceNets_WACV21-Supp.pdf">supp</a>
    /
    <a href="https://www.youtube.com/watch?v=kBx2_zGvkAA">video</a>
    <p></p>
    <em>Key words: 3D object detection, segmentation, classification, leverage 2D to 3D</em>
    <p>
    This work addresses the challenging task of automated object detection in aviation screening with 3D computed tomography scans.
    We propose a slice-and-fuse framework, that slices 3D volumes into 2D slices, leverages efficient 2D detectors (e.g. RetinaNet), 
    and fuse 2D predictions back to 3D. 
    </p>
  </td>
</tr>          


<tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='sv_sensor_img'><video  width=100% muted autoplay loop>
      <source src="images/sv_sensor.png" type="video/mp4">
      Your browser does not support the video tag.
      </video></div>
      <img src='images/sv_sensor.png' width=100%>
    </div>
    <script type="text/javascript">
      function nuvo_start() {
        document.getElementById('sv_sensor_img').style.opacity = "1";
      }

      function nuvo_stop() {
        document.getElementById('sv_sensor_img').style.opacity = "0";
      }
      nuvo_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2507.04190">
      <span class="papertitle">Towards Spatially-Varying Gain and Binning</span>
    </a>
    <br>
    <strong>Anqi Yang</strong>,
    Eunhee Kang,
    Wei Chen,
    Hyong-Euk Lee,
    Aswin C. Sankaranarayanan
    <br>
    <a href="https://arxiv.org/pdf/2507.04190"><em>arxiv</em>, 2025</a>
    <br>
    <p></p>
    <!-- <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
    /
    <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
    /
    <a href="http://arxiv.org/abs/2312.05283">arXiv</a> -->
    <em>Key words: Camera noise modeling, image sensors</em>
    <p>
    A conceptual image sensor with spatially-varying amplification and pixel sizes to enhance the noise performance and dynamic range.
    </p>
  </td>
</tr>


    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='occludedface_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/occludedface.jpg" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/occludedface.jpg' width="160">
        </div>
        <script type="text/javascript">
          function occludedface_start() {
            document.getElementById('occludedface_image').style.opacity = "1";
          }

          function occludedface_stop() {
            document.getElementById('occludedface_image').style.opacity = "0";
          }
          occludedface_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/8756594">
          <span class="papertitle">Pain Expression Recognition using Occluded Face</span>
        </a>
        <br>
        Ahmed Ashraf,
        <strong>Anqi Yang</strong>,
        Babak Taati
        <br>
        <em>FG</em>, 2019 &nbsp <font color="red"><strong>(Best Poster Award)</strong></font>
        <br>
        <a href="https://ieeexplore.ieee.org/document/8756594">paper</a>
        <p></p>
        <em>Key words: Facial landmarks, biometrics, machine learning for regression and classification</em>
        <p>
          This paper presents a preliminary study on automatic recognition of facial expression
          in ICU settings, where the patient face is partially occluded by surgery masks.
        </p>
      </td>
    </tr>

    
      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='palmprint_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/palmprint.jpg" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/palmprint.jpg' width="160">
          </div>
          <script type="text/javascript">
            function palmprint_start() {
              document.getElementById('palmprint_image').style.opacity = "1";
            }

            function palmprint_stop() {
              document.getElementById('palmprint_image').style.opacity = "0";
            }
            palmprint_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320317301681">
            <span class="papertitle">Towards Contactless Palmprint Recognition: A Novel Device, A New Benchmark, and A Collaborative Representation Based Identification Approach</Base></span>
          </a>
          <br>
          Lin Zhang,
          Lida Li,
          <strong>Anqi Yang</strong>,
          Ying Shen,
          Meng Yang
          <br>
          <em>Pattern Recognition</em>, 2017
          <br>
          <a href="https://cslinzhang.github.io/ContactlessPalm/">project page</a>
          /
          <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320317301681">paper</a>
          <p></p>
          <em>Key words: Biometric recognition, sparse representation, classification</em>
          <p>
          This paper presents a novel contacless palmprint acquisition system, a large-scale palmprint benchmark, 
          as well as a novel machine learning-based palmprint classification algorithm.
          </p>
        </td>
      </tr>

            

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Awards</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/awards.png" width="120"></td>
              <td width="75%" valign="center">
                ICCP Best Paper Award, 2021
                <br>
                Liang Ji-Dian Fellowship, 2022
                <br>
                Tan Endowed Graduate Fellowship, 2022
                <br>
                Google Anita Borg Memorial Scholarship, 2016
              </td>
            </tr>
            


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/review.jpeg" width="160" alt="cs188">
              </td>
              <td width="75%" valign="center">
                Recitation TA, CMU-18290 Signals and Systems
                <br>
                Recitation TA, CMU-18793 Image and Video Processing
                <br>
                Reviewer for TPAMI, TIP, CVPR, ICCV, ECCV, BMVC, WACV
              </td>
            </tr>
          
            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Last updated: Sept. 24, 2025.</p>
                <p style="text-align:right;font-size:small;">
                  This website is adapted from <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.</p>
              </td>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </table>
  </body>
</html>
